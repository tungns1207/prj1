{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d7bebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import cac thu vien can thiet\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b0df60f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 dong dau du lieu\n",
      "   AGE  SEX   BMI     BP   S1     S2    S3   S4      S5  S6    Y\n",
      "0   59    2  32.1  101.0  157   93.2  38.0  4.0  4.8598  87  151\n",
      "1   48    1  21.6   87.0  183  103.2  70.0  3.0  3.8918  69   75\n",
      "2   72    2  30.5   93.0  156   93.6  41.0  4.0  4.6728  85  141\n",
      "3   24    1  25.3   84.0  198  131.4  40.0  5.0  4.8903  89  206\n",
      "4   50    1  23.0  101.0  192  125.4  52.0  4.0  4.2905  80  135\n",
      "0    151\n",
      "1     75\n",
      "2    141\n",
      "3    206\n",
      "4    135\n",
      "Name: Y, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#load du lieu\n",
    "\n",
    "file_path='linear_data/diabetes.tab.txt'\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(file_path,sep='\\t')\n",
    "    \n",
    "    print(\"5 dong dau du lieu\")\n",
    "    print(df.head())\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"duong dan khong dung hoac khong tim thay file\")\n",
    "except Exception as e:\n",
    "    print(\"da gap loi {e}\")\n",
    "\n",
    "\n",
    "#tach du lieu train va label\n",
    "X = df.drop('Y', axis=1)\n",
    "y = df['Y']\n",
    "print(y.head())\n",
    "#chuyen tu dataframe qua numpy array\n",
    "X = X.to_numpy()\n",
    "y = y.to_numpy()\n",
    "\n",
    "#print(y.head())\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edb51749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.80050009  1.06548848  1.29708846 ...  0.41853093 -0.37098854\n",
      "   1.        ]\n",
      " [-0.03956713 -0.93853666 -1.08218016 ... -1.43658851 -1.93847913\n",
      "   1.        ]\n",
      " [ 1.79330681  1.06548848  0.93453324 ...  0.06015558 -0.54515416\n",
      "   1.        ]\n",
      " ...\n",
      " [ 0.87686984  1.06548848 -0.33441002 ... -0.98564884  0.32567395\n",
      "   1.        ]\n",
      " [-0.9560041  -0.93853666  0.82123474 ...  0.93616291 -0.54515416\n",
      "   1.        ]\n",
      " [-0.9560041  -0.93853666 -1.53537419 ... -0.08875225  0.06442552\n",
      "   1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Normalize data\n",
    "\n",
    "X_mean = np.mean(X,axis=0)\n",
    "X_std = np.std(X,axis=0)\n",
    "X_norma = (X-X_mean) / X_std\n",
    "\n",
    "#print(X_norma)\n",
    "\n",
    "\n",
    "#building matrix data\n",
    "one = np.ones((X.shape[0],1))\n",
    "\n",
    "X_data=np.concatenate((X_norma,one),axis=1)\n",
    "print(X_data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y, test_size=0.2, random_state=42)\n",
    "#print(X_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3737dad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cac he so cua duong thang la:\n",
      "a: [  1.80291044 -11.50907105  25.80070495  16.53858529 -44.30640613\n",
      "  24.64170961   7.77309591  13.09553713  35.01740872   2.31502709]\n",
      " b :  151.34560453985998\n",
      "ham loss RMSE =  53.55884336723094\n"
     ]
    }
   ],
   "source": [
    "# linear close form\n",
    "\n",
    "\n",
    "A = np.dot(X_train.T,X_train)\n",
    "b = np.dot(X_train.T,y_train)\n",
    "\n",
    "w_CF = np.dot(np.linalg.pinv(A),b)\n",
    "#print(w_CF.shape)\n",
    "\n",
    "print(\"cac he so cua duong thang la:\")\n",
    "print(\"a:\",w_CF[:X.shape[1]])\n",
    "print(\" b : \",w_CF[X.shape[1]])\n",
    "\n",
    "\n",
    "\n",
    "#loss function\n",
    "y_pred = np.dot(X_train,w_CF)\n",
    "\n",
    "\n",
    "loss_RMSE = math.sqrt(np.mean((y_pred-y_train)**2))\n",
    "print(\"ham loss RMSE = \",loss_RMSE)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e62ac29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss func tai lan thu 0 la: 28990.73952549963\n",
      "loss func tai lan thu 500 la: 3056.3357357842883\n",
      "loss func tai lan thu 1000 la: 2898.207433798493\n",
      "loss func tai lan thu 1500 la: 2895.467722426135\n",
      "loss func tai lan thu 2000 la: 2894.220204737955\n",
      "loss func tai lan thu 2500 la: 2893.0585210800223\n",
      "loss func tai lan thu 3000 la: 2891.9508213624185\n",
      "loss func tai lan thu 3500 la: 2890.8933667765777\n",
      "loss func tai lan thu 4000 la: 2889.883776538108\n",
      "loss func tai lan thu 4500 la: 2888.9198562785323\n",
      "loss func tai lan thu 5000 la: 2887.9995235261003\n",
      "loss func tai lan thu 5500 la: 2887.120796125505\n",
      "loss func tai lan thu 6000 la: 2886.2817855831786\n",
      "loss func tai lan thu 6500 la: 2885.4806915304493\n",
      "loss func tai lan thu 7000 la: 2884.715796857459\n",
      "loss func tai lan thu 7500 la: 2883.985463346027\n",
      "loss func tai lan thu 8000 la: 2883.2881276904673\n",
      "loss func tai lan thu 8500 la: 2882.622297828807\n",
      "loss func tai lan thu 9000 la: 2881.986549529269\n",
      "loss func tai lan thu 9500 la: 2881.3795231923245\n",
      "loss func tai lan thu 10000 la: 2880.7999208392666\n",
      "loss func tai lan thu 10500 la: 2880.2465032656505\n",
      "loss func tai lan thu 11000 la: 2879.7180873430766\n",
      "loss func tai lan thu 11500 la: 2879.213543456418\n",
      "loss func tai lan thu 12000 la: 2878.731793066086\n",
      "loss func tai lan thu 12500 la: 2878.2718063867947\n",
      "loss func tai lan thu 13000 la: 2877.832600175542\n",
      "loss func tai lan thu 13500 la: 2877.413235622508\n",
      "loss func tai lan thu 14000 la: 2877.012816339279\n",
      "loss func tai lan thu 14500 la: 2876.6304864393665\n",
      "loss func tai lan thu 15000 la: 2876.265428706391\n",
      "loss func tai lan thu 15500 la: 2875.916862845689\n",
      "loss func tai lan thu 16000 la: 2875.5840438153523\n",
      "loss func tai lan thu 16500 la: 2875.2662602329715\n",
      "loss func tai lan thu 17000 la: 2874.962832854575\n",
      "loss func tai lan thu 17500 la: 2874.6731131224233\n",
      "loss func tai lan thu 18000 la: 2874.3964817785113\n",
      "loss func tai lan thu 18500 la: 2874.132347540772\n",
      "loss func tai lan thu 19000 la: 2873.880145839136\n",
      "loss func tai lan thu 19500 la: 2873.6393376087162\n",
      "loss func tai lan thu 20000 la: 2873.40940813754\n",
      "loss func tai lan thu 20500 la: 2873.1898659663475\n",
      "loss func tai lan thu 21000 la: 2872.9802418381023\n",
      "loss func tai lan thu 21500 la: 2872.780087694972\n",
      "loss func tai lan thu 22000 la: 2872.5889757206164\n",
      "loss func tai lan thu 22500 la: 2872.4064974257553\n",
      "loss func tai lan thu 23000 la: 2872.23226277504\n",
      "loss func tai lan thu 23500 la: 2872.0658993533766\n",
      "loss func tai lan thu 24000 la: 2871.907051569911\n",
      "loss func tai lan thu 24500 la: 2871.755379897971\n",
      "loss func tai lan thu 25000 la: 2871.6105601493564\n",
      "loss func tai lan thu 25500 la: 2871.4722827814003\n",
      "loss func tai lan thu 26000 la: 2871.3402522353445\n",
      "loss func tai lan thu 26500 la: 2871.2141863046027\n",
      "loss func tai lan thu 27000 la: 2871.0938155315607\n",
      "loss func tai lan thu 27500 la: 2870.978882631627\n",
      "loss func tai lan thu 28000 la: 2870.8691419433094\n",
      "loss func tai lan thu 28500 la: 2870.7643589031213\n",
      "loss func tai lan thu 29000 la: 2870.6643095442255\n",
      "loss func tai lan thu 29500 la: 2870.568780017715\n",
      "loss func tai lan thu 30000 la: 2870.477566135527\n",
      "loss func tai lan thu 30500 la: 2870.390472934003\n",
      "loss func tai lan thu 31000 la: 2870.307314257168\n",
      "loss func tai lan thu 31500 la: 2870.227912358836\n",
      "loss func tai lan thu 32000 la: 2870.152097522684\n",
      "loss func tai lan thu 32500 la: 2870.079707699501\n",
      "loss func tai lan thu 33000 la: 2870.010588160813\n",
      "loss func tai lan thu 33500 la: 2869.9445911681646\n",
      "loss func tai lan thu 34000 la: 2869.8815756573363\n",
      "loss func tai lan thu 34500 la: 2869.8214069368305\n",
      "loss func tai lan thu 35000 la: 2869.763956399973\n",
      "loss func tai lan thu 35500 la: 2869.709101250032\n",
      "loss func tai lan thu 36000 la: 2869.6567242377414\n",
      "loss func tai lan thu 36500 la: 2869.6067134106916\n",
      "loss func tai lan thu 37000 la: 2869.558961874038\n",
      "loss func tai lan thu 37500 la: 2869.5133675620195\n",
      "loss func tai lan thu 38000 la: 2869.469833019802\n",
      "loss func tai lan thu 38500 la: 2869.4282651951717\n",
      "loss func tai lan thu 39000 la: 2869.388575239646\n",
      "loss func tai lan thu 39500 la: 2869.350678318561\n",
      "loss func tai lan thu 40000 la: 2869.314493429748\n",
      "loss func tai lan thu 40500 la: 2869.279943230394\n",
      "loss func tai lan thu 41000 la: 2869.246953871725\n",
      "loss func tai lan thu 41500 la: 2869.215454841163\n",
      "loss func tai lan thu 42000 la: 2869.185378811608\n",
      "loss func tai lan thu 42500 la: 2869.15666149753\n",
      "loss func tai lan thu 43000 la: 2869.1292415175644\n",
      "loss func tai lan thu 43500 la: 2869.1030602633114\n",
      "loss func tai lan thu 44000 la: 2869.078061774067\n",
      "loss func tai lan thu 44500 la: 2869.054192617209\n",
      "loss func tai lan thu 45000 la: 2869.0314017739893\n",
      "loss func tai lan thu 45500 la: 2869.009640530483\n",
      "loss func tai lan thu 46000 la: 2868.988862373468\n",
      "loss func tai lan thu 46500 la: 2868.9690228910026\n",
      "loss func tai lan thu 47000 la: 2868.9500796775023\n",
      "loss func tai lan thu 47500 la: 2868.9319922430973\n",
      "loss func tai lan thu 48000 la: 2868.91472192709\n",
      "loss func tai lan thu 48500 la: 2868.898231815318\n",
      "loss func tai lan thu 49000 la: 2868.8824866612554\n",
      "loss func tai lan thu 49500 la: 2868.8674528106735\n"
     ]
    }
   ],
   "source": [
    "#linear gradient descent\n",
    "# dinh nghia cac ham tinh gradient va loss\n",
    "def gradient(X_data, w_GD, y):\n",
    "    n = len(y)\n",
    "    y_pred = np.dot(X_data,w_GD)\n",
    "    #print(\"size\",y_pred.flatten().shape,y.shape)\n",
    "    #print(X_data.T.shape)\n",
    "    return np.dot(X_data.T,(y_pred.flatten() - y))* (1/n)\n",
    "\n",
    "#ham tinh loss la ham MSE\n",
    "def calculate_loss(X_data, w, y):\n",
    "    y_pred = np.dot(X_data, w)\n",
    "    \n",
    "    return np.mean((y_pred-y)**2)\n",
    "\n",
    "\n",
    "#khoi tao w ngau nhien \n",
    "w_GD = np.ones(X_train.shape[1])\n",
    "\n",
    "learning_rate = 0.005\n",
    "iteration = 50000\n",
    "\n",
    "\n",
    "for i in range(iteration):\n",
    "    grad = gradient(X_train,w_GD,y_train)\n",
    "    if(i%500==0):\n",
    "        los = calculate_loss(X_train, w_GD , y_train)\n",
    "        print(\"loss func tai lan thu\",i,\"la:\",los)\n",
    "    if(np.linalg.norm(grad) < 1e-3): break\n",
    "    \n",
    "    w_GD = w_GD - grad * learning_rate\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04f523f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11,)\n",
      "cac he so cua duong thang la \n",
      "a: [  1.82579904 -11.50062246  25.87965779  16.51373014 -40.23852368\n",
      "  21.45023638   5.94849872  12.53492871  33.45832275   2.34405921]\n",
      " b :  151.3412061349013\n"
     ]
    }
   ],
   "source": [
    "#hien thi cac thong so\n",
    "print(w_GD.shape)\n",
    "print(\"cac he so cua duong thang la \")\n",
    "print(\"a:\",w_GD[:X.shape[1]])\n",
    "print(\" b : \",w_GD[X.shape[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31e168f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "so sanh w cua cac thuat toan khac nhau:\n",
      "scikit-learn :  [  1.80291044 -11.50907105  25.80070495  16.53858529 -44.30640613\n",
      "  24.64170961   7.77309591  13.09553713  35.01740872   2.31502709\n",
      " 151.34560454]\n",
      "close-form:  [  1.80291044 -11.50907105  25.80070495  16.53858529 -44.30640613\n",
      "  24.64170961   7.77309591  13.09553713  35.01740872   2.31502709\n",
      " 151.34560454]\n",
      "gradient descent:  [  1.82579904 -11.50062246  25.87965779  16.51373014 -40.23852368\n",
      "  21.45023638   5.94849872  12.53492871  33.45832275   2.34405921\n",
      " 151.34120613]\n",
      "so sanh loss cua cac thuat toan khac nhau:\n",
      "scikit-learn: 2868.5497028355776\n",
      "close form :  2868.549702835578\n",
      "gradient descent :  2868.8530981297085\n"
     ]
    }
   ],
   "source": [
    "#su dung thu vien de check ket qua da dung chua\n",
    "regr = linear_model.LinearRegression(fit_intercept=False) # fit_intercept = False vi minh da tu them cot 1 roi\n",
    "regr.fit(X_train, y_train)\n",
    "\n",
    "# so sanh w cua cac ket qua\n",
    "print(\"so sanh w cua cac thuat toan khac nhau:\")\n",
    "print( 'scikit-learn : ', regr.coef_ )\n",
    "print( 'close-form: ', w_CF.T)\n",
    "print( 'gradient descent: ', w_GD.T)\n",
    "\n",
    "\n",
    "#so sanh loss\n",
    "print(\"so sanh loss cua cac thuat toan khac nhau:\")\n",
    "print( 'scikit-learn:',calculate_loss(X_train,regr.coef_,y_train))\n",
    "print( 'close form : ',calculate_loss(X_train,w_CF.T,y_train))\n",
    "print( 'gradient descent : ',calculate_loss(X_train,w_GD.T,y_train))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e6f4d4be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " test loss: 2900.1936284934786\n"
     ]
    }
   ],
   "source": [
    "#loss tren tap test\n",
    "print(\" test loss:\",calculate_loss(X_test,w_CF.T,y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
